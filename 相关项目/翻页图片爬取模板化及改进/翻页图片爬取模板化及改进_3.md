> 更新功能：  
> 优化url解析，防止更换中文网址出现的解码问题  
> 优化url解析，防止文件夹名出现非法字符  
> 优化输入提示，防止输入值越界报错  
> 优化功能，同一文件夹下已下载过的项不再重复下载  

***
```
import os,re
import requests
from bs4 import BeautifulSoup
from multiprocessing import Process
from urllib.parse import quote
import string

#定义主地址和字典
url = 'http://www.m8jpg.com'
headers={"user-agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36"}
menulist = {1: "xinggang", 2: "meixiong", 3: "meitun", 4: "meitui"}
menuurl = {1: "/xinggan/", 2: "/meixiong/", 3: "/meitun/", 4: "/meitui/"}


def get_html(url):
    """
    网页解析
    """
    url = quote(url, safe=string.printable)
    res = requests.get(url, headers=headers)
    #可直接用res.content取得字节
    html=res.content
    return html


def menu():
    """
    菜单式选项
    """
    #确定页数限制范围，防止越界
    pagelim={}
    for i in range(1,5):
        print (i, menulist[i])
        url_index='%s%s'%(url,menuurl[i])
        html=get_html(url_index)
        soup=BeautifulSoup(html,'lxml')        
        soup=soup.find('div',class_="nav-links page_imges").find_all('a')
        #页数从最后一页的网址里提取
        pagenum=soup[-1].get('href')
        pagenum=pagenum[(pagenum.index('_')+1):pagenum.index('.')]
        pagelim[i]=int(pagenum)
    input('请按提示依次输入所选栏目数字以及页数范围(按任意键继续):\n')
    menusel=[]
    #根据页数限制，添加检查防止报错
    while True:
        while True:
            num=int(input('请输入所选栏目数字:\n'))
            if num in range(1,5):
                break
            print('请重新输入有效范围内的数字')
        while True:
            page1=int(input('请输入需要下载的起始页数:\n'))
            page2=int(input('请输入需要下载的终止页数:\n'))
            if (page1 and page2 in range(1,pagelim[num]+1)) and (page1<=page2):
                break
            print('(请重新输入有效范围内的页数\n此栏目最大页数为%s\n且起始页数不能大于终止页数)'%(pagelim[num]))
        jud=input('请输入"end"结束输入或输入其他键继续选择栏目:\n')
        menusel.append((num,page1,page2+1))
        if jud=='end':
            break
    #用元组列表存储输入值，分项，分页数区间       
    for imenu in menusel:
        num=imenu[0]
        pagelist=range(imenu[1],imenu[2])
        for page in pagelist:
            p=Process(target=process,args=(num,page))
            p.start()


def process(num,page):
    """
    载入进程
    """
    print ('正在进入...')
    print (num,menulist[num],'第%d页'%(page))
    #首页地址差异
    if page!=1:
        firurl='%s%sindex_%d.html'%(url,menuurl[num],page)
    else:
        firurl='%s%sindex.html'%(url,menuurl[num])
    print (firurl)
    #一级地址载入
    html = get_html(firurl)

    #用find方法取标签
    soup=BeautifulSoup(html,'lxml')
    soup=soup.find_all('li',class_="i_list list_n2")
    imgnamelist=[]
    securllist=[]
    #取得一级地址中的文件名称和二级地址
    for isoup in soup:
        imgname=validatetitle(isoup.find('a').get('title'))
        path = 'F:/BeautyPic/%s/%s'%(menulist[num],imgname)
        if os.path.exists(path):
            print('"%s"\n此项文件夹已存在,正在跳过...'%(path))
            continue
        imgnamelist.append(imgname)
        securl=url+isoup.find('a').get('href')
        securllist.append(securl)

    #先创建文件夹，再载入内容
    mkfolder(num,imgnamelist)
    loadpage(num,securllist,imgnamelist)


def mkfolder(num,imgnamelist):
    """
    创建文件夹
    """
    path = 'F:/BeautyPic/%s'%(menulist[num])
    for imgname in imgnamelist:
        ipath='%s/%s'%(path,imgname)
        os.makedirs(ipath)
        
        
def validatetitle(title):
    rstr = r"[\/\\\:\*\?\"\<\>\|]"          # '/ \ : * ? " < > |'
    new_title = re.sub(rstr, "_", title)    # 替换为下划线
    return new_title

       
def loadpage(num,securllist,imgnamelist):
    """
    载入页面
    """
    
    i = 0
    #二级地址载入，注意此处img/sec两个列表元素一一对应，i从0计数
    for securl in securllist:
        html = get_html(securl)
        soup=BeautifulSoup(html,'lxml')
        #取得每个文件名称对应的页数
        pagenum = soup.find('div',class_="nav-links page_imges").find('a').find_all('b')[-1].get_text()
        page_range=range(1,int(pagenum)+1)
        #此处用num_ex防止num参数传递报错
        for num_ex in page_range:
            thiurl=securl
            if num_ex!=1:
                thiurl='%s_%d.html'%(thiurl[:-5],num_ex)
            html=get_html(thiurl)
            soup=BeautifulSoup(html,'lxml')
            #二级地址取得的src标签中仍是短地址，加工成三级地址
            thiurl = soup.find('div',class_="image_div").find('img').get('src')
            imgurl=url+thiurl        
            writepage(num, imgurl, imgnamelist[i])
        i+=1
    print ('全部下载已完成...')


def writepage( num, imgurl, imgname):
    """
    下载数据
    """
    path = 'F:/BeautyPic/%s/%s/%s'%(menulist[num],imgname,imgurl[-10:])
    #三级地址，即图片地址载入
    image=get_html(imgurl)
    print('正在下载...')
    print(path)
    with open(path,'wb') as f: 
        f.write(image)
    print('当前项下载已完成...')
        
        
if __name__ == '__main__':
    menu()
    
```
***
### to be continued...

